sudo yum install -y yum-utils
sudo yum-config-manager --add-repo https://dl.fedoraproject.org/pub/epel/7/x86_64/ 
sudo yum install --nogpgcheck -y epel-release && sudo rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 
sudo rm /etc/yum.repos.d/dl.fedoraproject.org*


http://download.ceph.com/rpm-nautilus/el7/noarch/

username=cehper
useradd ${username}
echo 'a123456ceph'  | passwd --stdin ${username}
echo "${username} ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/${username}
chmod 0440 /etc/sudoers.d/${username}

echo 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCs4RVLNlZt5cGsZ976/v2QECzL3eUy/ID4YEjtcAwHy269HQyCuL8HYEuiBeKD1gZaxKKcsPMy7vaOWsWcojeN7QJE6j9ILV/dJhYqfeX1NNBeMo0Bs3w7L1lncdGlnZSlWq4ZXTd0V99f9GuyibON1n9YaxD0SCDbuAngvgjJY/PAd0ljB7QkfEb6nyJm0rjGuTScKSRPshMaNMshWEnDtMAJU+hIMxt+/d6Z3u942CtRPx0M7YFMfKKG9+dbD2xeSbdixlMKyS+rgrSeolNrW7yDj7wM2nvO+4Z1jhQc6EmhSln5Q5CNCn9TyVfMPmU4jxIu/QtMgwHVnNHoXxJH cehper@tg-ops-sz001.tgops.com' > /home/cehper/.ssh/authorized_keys && chmod 600 /home/cehper/.ssh/authorized_keys


ceph-deploy install ceph-admin ceph-node1 ceph-node2 ceph-node3


RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite

----> ceph-deploy --overwrite-conf config push ceph-admin ceph-node1 ceph-node2

rbd create rbd3 --size 512 -p rbd

[root@tg-ops-sz004 ceph]# rbd map --image rbd3
rbd: sysfs write failed
RBD image feature set mismatch. You can disable features unsupported by the kernel with "rbd feature disable rbd3 object-map fast-diff deep-flatten".
In some cases useful info is found in syslog - try "dmesg | tail".
rbd: map failed: (6) No such device or address
[root@tg-ops-sz004 ceph]# rbd feature disable rbd3 exclusive-lock object-map deep-flatten fast-diff
[root@tg-ops-sz004 ceph]#
[root@tg-ops-sz004 ceph]#
[root@tg-ops-sz004 ceph]#
[root@tg-ops-sz004 ceph]# rbd map --image rbd3
/dev/rbd0


This means the image is still open or the client using it crashed. Try again after closing/unmapping it or waiting 30s for the crashed client to timeout.




[cehper@tg-ops-sz004 ~]$ rbd trash ls rbd
[cehper@tg-ops-sz004 ~]$ rbd trash mv rbd/rbd3
[cehper@tg-ops-sz004 ~]$ rbd trash ls rbd
11e76b8b4567 rbd3

[cehper@tg-ops-sz004 ~]$ rbd trash ls rbd
11e76b8b4567 rbd3
[cehper@tg-ops-sz004 ~]$ rbd trash restore 11e76b8b4567

$ rbd rm rbd/rbd3
2019-08-05 16:10:07.966 7f5ed37fe700 -1 librbd::image::RemoveRequest: 0x55d65e8698b0 check_image_watchers: image has watchers - not removing
Removing image: 0% complete...failed.
rbd: error: image still has watchers
This means the image is still open or the client using it crashed. Try again after closing/unmapping it or waiting 30s for the crashed client to timeout.

[root@tg-ops-sz004 ~]# rbd device unmap /dev/rbd0

[root@tg-ops-sz004 ~]# rbd ls rbd
rbd3
[root@tg-ops-sz004 ~]# rbd rm rbd/rbd3
Removing image: 100% complete...done.
[root@tg-ops-sz004 ~]# rbd ls rbd






[root@tg-ops-sz004 ~]# rbd create --size 1024 rbd/rbd4
[root@tg-ops-sz004 ~]# rbd ls
rbd4
[root@tg-ops-sz004 ~]# rbd ls rbd
rbd4
[root@tg-ops-sz004 ~]# rbd info rbd4
rbd image 'rbd4':
	size 1 GiB in 256 objects
	order 22 (4 MiB objects)
	id: 12d26b8b4567
	block_name_prefix: rbd_data.12d26b8b4567
	format: 2
	features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
	op_features:
	flags:
	create_timestamp: Mon Aug  5 16:13:31 2019


RESIZING A BLOCK DEVICE IMAGE

[root@tg-ops-sz004 ~]# rbd resize --size 2048 rbd/rbd4
Resizing image: 100% complete...done.
[root@tg-ops-sz004 ~]# rbd info rbd4
rbd image 'rbd4':
	size 2 GiB in 512 objects
	order 22 (4 MiB objects)
	id: 12d26b8b4567
	block_name_prefix: rbd_data.12d26b8b4567
	format: 2
	features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
	op_features:
	flags:
	create_timestamp: Mon Aug  5 16:13:31 2019

REMOVING A BLOCK DEVICE IMAGE

[root@tg-ops-sz004 ~]# rbd rm rbd/rbd4
Removing image: 100% complete...done.
[root@tg-ops-sz004 ~]# rbd ls rbd

To defer delete a block device from a pool, execute the following, but replace {image-name} with the name of the image to move and replace {pool-name} with the name of the pool:

[root@tg-ops-sz004 ~]# rbd create --size 1024 rbd/rbd5
[root@tg-ops-sz004 ~]# rbd ls rbd
rbd5
[root@tg-ops-sz004 ~]# rbd trash mv rbd/rbd5
[root@tg-ops-sz004 ~]# rbd trash ls
131c6b8b4567 rbd5
[root@tg-ops-sz004 ~]# rbd trash rm rbd/131c6b8b4567
Removing image: 100% complete...done.

RESTORING A BLOCK DEVICE IMAGE 

[cehper@tg-ops-sz004 ~]$ rbd trash mv rbd/rbd3
[cehper@tg-ops-sz004 ~]$ rbd trash ls rbd
11e76b8b4567 rbd3

[cehper@tg-ops-sz004 ~]$ rbd trash ls rbd
11e76b8b4567 rbd3
[cehper@tg-ops-sz004 ~]$ rbd trash restore 11e76b8b4567


To restore a deferred delete block device in the rbd pool, execute the following, but replace {image-id} with the id of the image:

rbd trash restore {image-id}
For example:

rbd trash restore 2bf4474b0dc51
To restore a deferred delete block device in a particular pool, execute the following, but replace {image-id} with the id of the image and replace {pool-name} with the name of the pool:

rbd trash restore {pool-name}/{image-id}
For example:

rbd trash restore swimmingpool/2bf4474b0dc51
You can also use --image to rename the image while restoring it.

For example:

rbd trash restore swimmingpool/2bf4474b0dc51 --image new-name



CREATE A BLOCK DEVICE POOL

On the admin node, use the ceph tool to create a pool.




Error EPERM: WARNING: this will *PERMANENTLY DESTROY* all data stored in pool rbd.  If you are *ABSOLUTELY CERTAIN* that is what you want, pass the pool name *twice*, followed by --yes-i-really-really-mean-it.


vim ceph.conf

ceph-deploy --overwrite-conf config push tg-ops-sz001 tg-ops-sz002 tg-ops-sz003 

systemctl restart ceph-mon.target

$ ceph osd pool delete cephfs-test cephfs-test --yes-i-really-really-mean-it
pool 'cephfs-test' removed



LIST POOLS

ceph osd lspools

CREATE A POOL

[cehper@tg-ops-sz001 my-cluster]$ ceph osd pool create tgops 128
pool 'tgops' created
[cehper@tg-ops-sz001 my-cluster]$ ceph osd lspools
3 default.rgw.control
4 default.rgw.meta
5 default.rgw.log
7 tgops

SET THE NUMBER OF OBJECT REPLICAS

 [cehper@tg-ops-sz001 my-cluster]$ ceph osd dump | grep tgops
pool 7 'tgops' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 last_change 40 flags hashpspool stripe_width 0
[cehper@tg-ops-sz001 my-cluster]$ ceph osd pool set tgops size 2
set pool 7 size to 2
[cehper@tg-ops-sz001 my-cluster]$ ceph osd dump | grep tgops
pool 7 'tgops' replicated size 2 min_size 2 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 last_change 43 flags hashpspool stripe_width 0


错误提示：

[cehper@tg-ops-sz001 ~]$ ceph -s
  cluster:
    id:     656dc120-29b7-402a-804e-e6fa29ef1601
    health: HEALTH_WARN
            1 pools have pg_num > pgp_num


[cehper@tg-ops-sz001 ~]$ ceph osd pool set .rgw.root pg_num  3
Error EEXIST: specified pg_num 3 <= current 8                      -------> 需要注意, pg_num只能增加, 不能缩小.
[cehper@tg-ops-sz001 ~]$ ceph osd pool set .rgw.root pgp_num  8    -------> 那这里就只能增加pgp_num数值
set pool 9 pgp_num to 8

[cehper@tg-ops-sz001 ~]$ ceph -s
  cluster:
    id:     656dc120-29b7-402a-804e-e6fa29ef1601
    health: HEALTH_OK



SET POOL QUOTAS

[cehper@tg-ops-sz001 my-cluster]$ ceph osd pool set-quota tgops max_objects 100
set-quota max_objects = 100 for pool tgops


RENAME A POOL

[cehper@tg-ops-sz001 my-cluster]$ ceph osd pool rename tgops new-tgops
Error EEXISTpool 'tgops' renamed to 'new-tgops'
[cehper@tg-ops-sz001 my-cluster]$ ceph osd lspools
3 default.rgw.control
4 default.rgw.meta
5 default.rgw.log
7 new-tgops

SHOW POOL STATISTICS

[cehper@tg-ops-sz001 my-cluster]$ rados df
POOL_NAME           USED OBJECTS CLONES COPIES MISSING_ON_PRIMARY UNFOUND DEGRADED RD_OPS      RD WR_OPS  WR
default.rgw.control  0 B       8      0     24                  0       0        8      0     0 B      0 0 B
default.rgw.log      0 B     207      0    621                  0       0      207 168522 164 MiB 112284 0 B
default.rgw.meta     0 B       0      0      0                  0       0        0      0     0 B      0 0 B
new-tgops            0 B       0      0      0                  0       0        0      0     0 B      0 0 B

total_objects    215
total_used       2.0 GiB
total_avail      38 GiB
total_space      40 GiB


GET POOL VALUES

[cehper@tg-ops-sz001 my-cluster]$ ceph osd pool get tgops size
size: 2
[cehper@tg-ops-sz001 my-cluster]$ ceph osd pool get tgops pg_num
pg_num: 128

MAKE A SNAPSHOT OF A POOL

[cehper@tg-ops-sz001 my-cluster]$ ceph osd pool mksnap tgops snap-tgops
created pool tgops snap snap-tgops

查看快照 


[cehper@tg-ops-sz001 my-cluster]$ rados lssnap -p tgops

1	snap-tgops	2019.08.06 10:36:08
1 snaps


演示ceph的snapshot功能。我们往pool里写入一个对象，然后获取pool的快照，接着删除对象，最后从快照里恢复数据

[cehper@tg-ops-sz001 my-cluster]$ rados -p tgops put testfile /etc/hosts    

[cehper@tg-ops-sz001 my-cluster]$
[cehper@tg-ops-sz001 my-cluster]$ rados -p tgops ls
testfile
[cehper@tg-ops-sz001 my-cluster]$ rados mksnap snap-tgops-1 -p tgops
created pool tgops snap snap-tgops-1
[cehper@tg-ops-sz001 my-cluster]$ rados lssnap -p tgops
1	snap-tgops	2019.08.06 10:36:08
2	snap-tgops-1	2019.08.06 10:44:27
2 snaps
[cehper@tg-ops-sz001 my-cluster]$ rados -p tgops rm testfile
[cehper@tg-ops-sz001 my-cluster]$ rados -p tgops rm testfile
error removing tgops>testfile: (2) No such file or directory
[cehper@tg-ops-sz001 my-cluster]$ rados -p tgops ls
testfile
[cehper@tg-ops-sz001 my-cluster]$ rados -p tgops rm testfile
error removing tgops>testfile: (2) No such file or directory
[cehper@tg-ops-sz001 my-cluster]$ rados rollback -p tgops testfile snap-tgops-1
rolled back pool tgops to snapshot snap-tgops-1
[cehper@tg-ops-sz001 my-cluster]$ rados -p tgops ls
testfile
[cehper@tg-ops-sz001 my-cluster]$ rados -p tgops rm testfile





=========================
BASIC BLOCK DEVICE COMMANDS
=========================

CREATE A BLOCK DEVICE POOL

On the admin node, use the ceph tool to create a pool.

On the admin node, use the rbd tool to initialize the pool for use by RBD:

rbd pool init <pool-name>

CREATING A BLOCK DEVICE IMAGE

[cehper@tg-ops-sz001 my-cluster]$ rbd pool init test001
[cehper@tg-ops-sz001 my-cluster]$ rbd create --size 4096 test001/rbd001
[cehper@tg-ops-sz001 my-cluster]$ rbd ls test001
rbd001

To list deferred delete block devices in the rbd pool, execute the following:

rbd trash ls {poolname}


RETRIEVING IMAGE INFORMATION

[cehper@tg-ops-sz001 my-cluster]$ rbd info test001/rbd001
rbd image 'rbd001':
	size 4 GiB in 1024 objects
	order 22 (4 MiB objects)
	id: 5f596b8b4567
	block_name_prefix: rbd_data.5f596b8b4567
	format: 2
	features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
	op_features:
	flags:
	create_timestamp: Tue Aug  6 11:19:03 2019

RESIZING A BLOCK DEVICE IMAGE

[cehper@tg-ops-sz001 my-cluster]$ rbd resize --size 5120 test001/rbd001
Resizing image: 100% complete...done.
[cehper@tg-ops-sz001 my-cluster]$ rbd info test001/rbd001
rbd image 'rbd001':
	size 5 GiB in 1280 objects
	order 22 (4 MiB objects)
	id: 5f596b8b4567
	block_name_prefix: rbd_data.5f596b8b4567
	format: 2
	features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
	op_features:
	flags:
	create_timestamp: Tue Aug  6 11:19:03 2019

REMOVING A BLOCK DEVICE IMAGE

[cehper@tg-ops-sz001 my-cluster]$ rbd rm test001/rbd001
Removing image: 100% complete...done.

RESTORING A BLOCK DEVICE IMAGE

[cehper@tg-ops-sz001 my-cluster]$ rbd trash mv test001/rbd001
[cehper@tg-ops-sz001 my-cluster]$ rbd trash ls test001
5f806b8b4567 rbd001

[cehper@tg-ops-sz001 my-cluster]$ rbd trash restore test001/5f806b8b4567
[cehper@tg-ops-sz001 my-cluster]$ rbd ls test001
rbd001

===================
SNAPSHOTS
===================

CREATE SNAPSHOT

[cehper@tg-ops-sz001 my-cluster]$ rbd snap create test001/rbd001@snap-rbd001
[cehper@tg-ops-sz001 my-cluster]$ rbd snap ls test001/rbd001
SNAPID NAME         SIZE TIMESTAMP
     4 snap-rbd001 4 GiB Tue Aug  6 11:35:27 2019

ROLLBACK SNAPSHOT

[cehper@tg-ops-sz001 my-cluster]$ rbd snap rollback test001/rbd001@snap-rbd001
Rolling back to snapshot: 100% complete...done.

DELETE A SNAPSHOT

[cehper@tg-ops-sz001 my-cluster]$ rbd snap purge test001/rbd001
Removing all snapshots: 100% complete...done.
[cehper@tg-ops-sz001 my-cluster]$ rbd snap ls test001/rbd001



在客户端映射 

# rbd map --image test001/rbd001

rbd: sysfs write failed
RBD image feature set mismatch. You can disable features unsupported by the kernel with "rbd feature disable rbd001 object-map fast-diff deep-flatten".
In some cases useful info is found in syslog - try "dmesg | tail".


# rbd feature disable test001/rbd001 exclusive-lock object-map deep-flatten fast-diff
# rbd showmapped
id pool    image  snap device
0  test001 rbd001 -    /dev/rbd0

挂载到本地 

# df -h /rbd
文件系统        容量  已用  可用 已用% 挂载点
/dev/rbd0       4.0G   33M  4.0G    1% /rbd

取消映射

# rbd unmap /dev/rbd0



ceph命令：

1. 查看osd 的目录树:  ceph osd tree
2. 查看机器的实时运行状态 ：ceph –w
3. 查看ceph的存储空间 ：ceph df
4. 查看mon的状态信息 ：ceph mon stat
5. 查看osd运行状态 : ceph osd stat
